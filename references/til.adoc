
## About

TIL standards for "Today I Learnt". The idea is by recording small things learnt their learning will be better retained. In the large I think it also provides a nice rough way of tracking progress every once in a while, and finding patterns in one's self that would otherwise be missed. This idea is not mine, but rather what I've seen others do e.g. https://github.com/jbranchaud/til/commits/master[here] https://github.com/thoughtbot/til[here] and https://github.com/milooy/TIL[here]. I think its a nifty idea worth emulating.

Items below are ordered latest to oldest. Look at the git history for the actual dates.

## TIL

**Tue Sep 19**

* In Go command line flags are processed such that `-a=x` `--a=x` `-a x` `--a=x` all mean the same thing. The only exception is that booleans `true`/`false` cannot be used with the form `-a x` or `--a x`.

* It turns out in dgraph the `--groups` flag doesn't require quotes around the arguments so `--groups="0,1"` can be entered as just `--groups=0,1`. This point matters because in the k8s deployment spec the former with quotes leads to a parse error in dgraph! This wasted several hours of my time...
+
```
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: dgraph
  labels:
    app: dgraph
spec: # ReplicaSet
  replicas: 1
  template: # Pod
    metadata:
      labels:
        app: dgraph
    spec:
      volumes:
        - name: group-mappings
          configMap:
            name: dgraph-group-mappings.conf
        - name: dgraph
          persistentVolumeClaim:
            claimName: mypvc
      containers:
        - name: dgraph
          image: dgraph/dgraph:latest
          ports:
            - name: ui
              containerPort: 8080
            - name: client
              containerPort: 9080
          command:
            - dgraph
          args:
            - --group_conf=/dgraph-config/group-mappings
            - --groups="0,1"
            - --idx=1
            - --bindall=true
            - --memory_mb=2048
          volumeMounts:
            - name: dgraph
              mountPath: /dgraph
            - name: group-mappings
              mountPath: /dgraph-config
```
```
❯ kc logs dgraph-3594762630-2bfm3

Dgraph version   : v0.8.1
Commit SHA-1     : 8ea4b0a5
Commit timestamp : 2017-09-18 10:56:37 +1000
Branch           : HEAD

2017/09/20 01:39:09 Unable to parse 'groups' configuration
```

'''
**Mon Sep 18**

* running a dgraph cluster
** multiple dgraph instances can run together in a cluster to scale read/write performance as well as provide high availability of the data
** additional configuration steps are necessary to setup a cluster
** each instance must be instructed to take on certain `group`s of predicates `--groups`
** a predicate-to-group mapping spec must be given to each instance `--group_conf`
** The mapping spec has some basic features like edge-name-prefix matching and `fp` variable for the so-called fingerprint of an edge

* in Kubernetes a ConfigMap resource type allows mounting file contents at file names given by the resource spec key names. for example:
+
```
spec:
  <filename_1_here>: |
    file 1 contents here!
  <filename_2_here>: |
    file 2 contents here!
```

**Sun Sep 17**

* In Kubernetes it is possible to maintain persistent data

** The core concepts are:
. `volume classes` (`vc`)
. `persistent volumes` (`pv`) based on `vc`
. persistent volume claims` (`pvc`) based on `pvc`
. `pod` specs that specify volumes based on `pvc`s
. `container` specs (within pod specs) that specify `volumeMounts` based on `pod` `volumes`

** So each of these is based upon the former until `vc` hits the raw layer of whatever the kubernetes is hosted upon.
** Since Kubernetes 1.6 it has been possible to create `pvc`s directly without needing to first create `pv`s. This is referred to as dynamic provisioning. http://blog.kubernetes.io/2017/03/dynamic-provisioning-and-storage-classes-kubernetes.html[link]
** More about the concept can be read on the https://kubernetes.io/docs/concepts/storage/volumes/[storage docs]. The third sub-section just links to a blog post, so incomplete?
** A blog post going over the topic in an end-to-end example can be found http://blog.bigbinary.com/2017/04/12/using-kubernetes-persistent-volume-for-persistent-data-storage.html[here]
** Another example is https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/[one section on the k8s task-oriented docs]
** Example:
+
```
❯ kc get pv
NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM           STORAGECLASS   REASON    AGE
pvc-46c2c0df-9c19-11e7-a0d1-0800271d32bc   1Gi        RWO           Delete          Bound     default/mypvc   standard                 47m

❯ kc get pvc
NAME      STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
mypvc     Bound     pvc-46c2c0df-9c19-11e7-a0d1-0800271d32bc   1Gi        RWO           standard       47m

❯ cat ./deployment.yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: dgraph
  labels:
    app: dgraph
spec: # ReplicaSet
  replicas: 1
  template: # Pod
    metadata:
      labels:
        app: dgraph
    spec:
      volumes:
        - name: dgraph
          persistentVolumeClaim:
            claimName: mypvc
      containers:
        - name: dgraph
          image: dgraph/dgraph:latest
          ports:
            - name: ui
              containerPort: 8080
            - name: client
              containerPort: 9080
          command:
            - dgraph
          args:
            - -bindall=true
            - -memory_mb=2048
          volumeMounts:
            - name: dgraph
              mountPath: /dgraph
```

**Fri Sep 15**

* installing dgraph https://docs.dgraph.io/get-started/#from-install-scripts[via simple bash script] makes not just `dgraph` available on the command line but also `dgraphloader`.
* data can be imported and exported out of dgraph using a file format called RDF. RDF stands for https://en.wikipedia.org/wiki/Resource_Description_Framework["resource description framework"]. It is actually a family of specifications maintained by the https://en.wikipedia.org/wiki/World_Wide_Web_Consortium[W3C]. N-Tripples are one of the common serialization formats for RDF data, and not coincidentally as I noted a few days ago tripples are also a W3C specification. The main enlightenment here was that I realized dgraph `set` syntax (`mutate { set { ... }}`) isn't its own design but rather just RDF. In fact the contents of an RDF file can be copy-pasted into this `set` block! It is not clear if the reverse is true. In otherwise RDF may just be a subset of what dgraph `set` can do.
* In dgraph there are no properties on nodes, just named edges to types of data
* In dgraph up until today it was only possible to have multiple outgoing node edges of the same name to other _nodes_, but not to other _values_. So for example if you had a product node it was not possible to attach multiple `image` edges to URL values. Each attachment would just override the previous one. On the other hand a person node could have multipe `friend` edges to other person nodes. However today a feature landed in `master` that allows multiple same-named edges to values just like nodes! https://dgraph.slack.com/archives/C13LH03RR/p1505509178000026[link]
* dgraph has an interface for making queries and visualizing their results +
+
image::./assets/dgraph-ui.png[]
* a dgraph schema is a non-nested map of edge names to types. The types are the type of value pointed _to_ by that edge. There are no namespaces. when we add `@index` to the typing we're making _any_ node with an _outgoing_ edge of the respective name available as an entry point (e.g. `foobar(func: allofterms(some_edge_here, "some value here"))`) or for filtering (e.g. `friend @filter(allofterms(some_edge_here, "some value here")) { ... }`).
* dgraph `@filter` and entrypoint are two syntaxes for doing the same thing it seems e.g. they each accept the same functions `allofterms` `anyofterms` `eq` ...
* When specifying a field in the schema design `@reverse` makes it possible to use `~field_name_here { ... }` in queries which will follow the edge back to where its pointing _from_. `~` is the special part that signifies to travel the edge in reverse. For example given a `product` node and `category` node and a `category` _edge_ from product to `category` it would be possible to do `~category { ...product fields here... }` within a category context in a query to get the product that points to it.
* given the lack of namespacing in dgraph schemas a convention has emerged to name edges with a prefix of the node type. For example in a movies database to differentiate directors from actors the schema used edge names `director.film` and `actor.film`. Its not clear how far this pattern should go. It seems like a case-by-case decision.



'''
**Sun Sep 10**

* found out that asciidoc does not support strikethough in a way that supports Github (or viceversa) https://github.com/asciidoctor/asciidoctor/issues/1030[link] https://github.com/christiangalsterer/bitbucket-asciidoc-plugin/issues/15[link]. This prevented me from being able to format a log title in the way I wanted.

* Amazon Alexa is a kind of voice-based interface not unlike Apple Siri.
** Amazon Echo is a hardware product line that makes Alexa convenient to use
** Developers can "teach Alexa skills" which is analogus to e.g. writing iOS apps. teach -> write, skill -> app
** Alexa skills are configured with an amazon developer account, then implemented. The skill's interaction model is defined in this configuration layer, e.g. what utterances can be used.
** `Invocation Name` is the name given to enter your skill from alexa. For example `essence` will enter the `ssense` skill
** Each skill has multiple `intents`. These are like functions or endpoints in your skill. You defined them as a developer.
** Each intent has multiple `utterances`. These are ways the user can speak to execute the intent.
** There is another concept called `slots` which are for parameters in intents. But I have not actually played with these yet.
** There are different APIs available for developers to use to build skills. For highly custom skills there is a Custom API which can POST intents to any host running an HTTPS server.
** links: https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/getting-started-guide[Alexa Skills entry point for developers], https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/build-skills-for-echo-show#display-and-interaction-features-on-echo-show[Amazon Echo Show entry point for developers], https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/overviews/understanding-custom-skills[Custom API], https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-interface-reference[JSON Interface Reference for Custom Skills], https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/display-interface-reference[Display Interface Reference]

* DGraph's https://godoc.org/github.com/dgraph-io/dgraph/client[go client] is their most feature complete one. DGraph's https://github.com/dgraph-io/dgraph/tree/master/cmd/dgraphloader[`dgraphloader`] is built on top of it.


'''
**Wed Sep 6**

* learnt about the following `dgraph` `mutation` today.
+
```
mutation {
  set {
   _:cat <name> "Millhouse" .
   _:cat <color> "Black" .
   _:cat <age> "0.7"^^<xs:float> .

   _:human <name> "Kaley" .
   _:human <age> "22"^^<xs:float> .
   _:human <favorite_food> "chocolate" .

   _:human <owns> _:cat .
  }

  schema {
   name: string @index .
  }
}
```
** `mutation` is for changing data in the graph or changing the graph schema
** `set` is for mutations that insert triples into the graph
** the strange syntax `^^<xs:float>` is apparently how a value is typed as a float...

* about `dgraph` triples
** triples are specified according the W3C standard https://www.w3.org/TR/n-quads/[RDF N-Quad format]
** their format is `<subject> <predicate> <object> .` `subject` is always a node. `object` is either a `node` or a `value` (also know as literal). `predicate` is a directed edge from `subject` to `object`, the value here is the edge name. A given edge must always point to a consistent type (in effect the edge type). A `.` is present because of the spec apparently less because of need on dgraph side https://dgraph.slack.com/archives/C13LH03RR/p1504754827000129[link]

* `blank node` is written `_:identifier` in a mutation. Used to identify a node within a mutation. Outside a particular mutation the identifiers have no existance. `_` will be replaced by dgraph with an automatically generated 64bit unique ID. These IDs are available in the mutation return result:
+
```
{
  "data": {
    "code": "Success",
    "message": "Done",
    "uids": {
      "foo": "0x2712",
      "qux": "0x2713",
      "bar": "0x2714"
    }
  }
}
```

* links: https://docs.dgraph.io/query-language/#mutations[mutation docs], https://docs.dgraph.io/master/guides/#adding-data-to-dgraph[guide/intro to mutations]

* in `dgraph` schema types are defined globally without any ability to nest into records. https://dgraph.slack.com/archives/C13LH03RR/p1504755357000113[link]. For example this would fail:
+
```
mutation {
  schema {
    foo {
      bar: string .
    }
  }
}
```
+
but this would work:
+
```
mutation {
  schema {
    bar: string .
  }
}
```




* `dgraph` supports pagination which can be used as the basis for doing batch work across an entire graph. https://dgraph.slack.com/archives/C13LH03RR/p1504745800000004[slack link], https://docs.dgraph.io/master/query-language/#pagination[pagination docs link]

'''
**Tue Sep 5**

* https://dgraph.io[dgraph] has enough power in its query language to apply both collaborative-based and content-based filtering strategies https://blog.dgraph.io/post/recommendation[link] https://blog.dgraph.io/post/recommendation2/[link].

* _collaborative-based filtering_ is a broad strategy for recommending things based upon matching like-users and then recommending to one based on another(s).

* _cold-start_ problem refers to being unable to integrate a new user into collaborative-based filtering for lack of data with that user.

* _content-based filtering_ is a broad strategy for recommending things based on their similarity to another given thing.
